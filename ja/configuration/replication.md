# データ複製

複製はすべてのGravwell Cluster Editionライセンスに含まれているため、フォールトトレラントな高可用性展開が可能です。Gravwell複製エンジンは、自動フェイルオーバー、負荷分散されたデータ配布、および圧縮によって、分散インデクサー間のデータ複製を透過的に管理します。Gravwellはまた、どのウェルが複製に含まれるのか、そしてデータがどのようにしてピアに分散されるのかを厳密に制御します。顧客は、均一なデータ分散でGravwellクラスターを迅速に展開することも、地域を意識したピア選択を使用してデータセンター全体の障害に耐えることができるレプリケーション方式を設計することもできます。オンラインフェイルオーバーシステムでは、一部のインデクサーがオフラインの場合でもデータへの継続的なアクセスが可能になります。

重要：Gravwellの複製システムは、取り込んだデータを複製するために純粋に設計されています。 インデクサーではなくウェブサーバーに保存されているユーザーアカウント、ダッシュボード、検索履歴、リソースなどはバックアップしません。 Webサーバーに復元力を追加するには、Webサーバーに[データストア](#!distributed/frontend.md)をデプロイすることを検討してください。 データストアには、ウェブサーバーのデータの冗長なライブコピーが保存されます。

レプリケーションシステムは論理的に「クライアント」と「ピア」に分けられ、各インデクサはピアとクライアントの両方として機能する可能性があります。クライアントは、既知の複製ピアに連絡を取り、複製トランザクションを推進する責任があります。Gravwellクラスターを複製モードでデプロイする場合、インデクサーが複製ストレージノードとして機能するすべてのピアへのTCP接続を開始できることが重要です。

レプリケーション接続は既定で暗号化されており、インデクサーにX509証明書が機能している必要があります。証明書が有効な認証局（CA）によって署名されていない場合は`Insecure-Skip-TLS-Verify=true`、複製構成セクションに追加する必要があります。

レプリケーションストレージノード（レプリケートされたデータを受信するノード）には特定の量のストレージが割り当てられ、そのストレージが使い果たされるまでデータは削除されません。リモートクライアントノードが通常のエージアウトの一環としてデータを削除した場合、レプリケーションノードがそのストレージ制限に達すると、データ断片は削除済みとしてマークされ、削除の優先順位が付けられます。複製システムは、削除された断片を優先し、次に冷たい断片を優先し、最も古い断片を最後に優先します。すべての複製データは圧縮されています。コールドストレージの場所が提供されている場合、レプリケーションストレージの場所は、コールドストレージとホットストレージを組み合わせたものと同じストレージ容量を持つことが通常推奨されます。

<span style="color: red; ">重要：999MBを超えるエントリーは複製されません。それらは通常通りに摂取および検索できますが、複製から除外されます。</span>

## 基本的なオンライン配置

最も基本的なレプリケーション展開は、すべてのインデクサーが他のすべてのインデクサーに対して複製できる均一な配布です。ユニフォーム展開は、[複製ピア]フィールドに他のすべてのインデクサーを指定することによって構成されます。

![Basic Replication](replicationOverview.png)

### 設定例

3つのインデクサー（192.168.100.50、192.168.100.51、192.168.100.52）があるとすると、192.168.100.50のインデクサーの構成は次のようになります。

```
[Replication]
	Peer=192.168.100.51
	Peer=192.168.100.52
	Storage-Location=/opt/gravwell/replication_storage
	Insecure-Skip-TLS-Verify=true
	Connect-Wait-Timeout=60
```
各ノードはその`Peer`フィールドに他のノードを指定します。

## 地域対応展開

複写システムは、インデクサーがどのピアにデータを複製できるかを微調整するように設定できます。レプリケーションピアを制御することで、オンラインのアベイラビリティーゾーンでその後の損失が発生しない限り、データを失うことなく領域全体をオフラインにすることができる可用性領域を設定できます。

![Region Aware Replication](RegionAwareReplication.png)

### 設定例

たとえば、8ノードのクラスタは、2つのアベイラビリティゾーン（1と2）に分割されます。利用可能ゾーン1のサブネットが172.16.2.0/24で、利用可能ゾーン2のサブネットが172.20.1.0/24の場合。

リージョン1のノードは、リージョン2に複製するように構成されています。

```
[Replication]
	Peer=172.20.1.101
	Peer=172.20.1.102
	Peer=172.20.1.103
	Peer=172.20.1.104
	Storage-Location=/opt/gravwell/replication_storage
	Connect-Wait-Timeout=60
```

リージョン2のノードは、リージョン1に複製するように構成されています。

```
[Replication]
	Peer=172.16.2.101
	Peer=172.16.2.102
	Peer=172.16.2.103
	Peer=172.16.2.104
	Storage-Location=/opt/gravwell/replication_storage
	Connect-Wait-Timeout=60
```

## オフライン展開
レプリケーションは、標準のシングルエディションGravwellライセンスには含まれていません。Gravwellをマルチノードで展開する必要がなく、管理されたバックアップのためにレプリケーションエンジンにアクセスしたい場合は、sales @ gravwell.ioに連絡して、Single EditionライセンスをレプリケートSingle Editionライセンスにアップグレードしてください。シングルエディションレプリケーションは完全にオフラインです。つまり、インデックス作成がオフラインになると、インデクサーがオンラインに戻って回復が完了するまでデータを検索できません。

![Single Edition Offline Replication](SingleOfflineReplication.png)

Cluster Edition Gravwellのライセンスでは、オフラインレプリケーターを使用してオフラインレプリケーション構成を実装することを選択できます。オフラインレプリケータはレプリケーションピアとして排他的に機能し、自動フェイルオーバーを提供したりインデクサとして機能したりすることはありません。オフラインレプリケーション構成は、ストレージシステムがすでに冗長ストアでバックアップされており、損失が発生する可能性が極めて低いクラウド環境で役立ちます。オフライン複製構成を使用すると、インデクサーとしては機能しない非常に低コストのストレージプールに接続されている低コストのインスタンスにデータを複製できます。万が一インデクサが完全に失われた場合、低コストの複製ピアがより高コストのインデクサインスタンスを復元できます。オフライン複製パッケージにアクセスするには、sales @ gravwell.ioに連絡してください。

![Offline Replication](OfflineReplicationCluster.png)

## 設定オプション

複製は、gravwell.conf構成ファイルの "複製"構成グループによって制御されます。レプリケーション構成グループには、以下の構成パラメーターがあります。

| パラメータ | 例 | 説明 |
|:----------|:--------|------------:|
| Peer      | Peer=10.0.0.1:9406 | Designates a remote system acting as a replication storage node.  Multiple Peers can be specified. |
| Connect-Wait-Timeout | Connect-Wait-Timeout=30 | Specifies the number of seconds an Indexer should wait when attempting to connect to replication peers during startup. |
| Listen-Address | Listen-Address=10.0.0.101:9406 | Designates the address to which the replication system should bind.  Default is to listen on all addresses on TCP port 9406. |
| Storage-Location | Storage-Location=/mnt/storage/gravwell/replication | Designates the full path to use for replication storage. |
| Max-Replicated-Data-GB | Max-Replicated-Data-GB=4096 | Designates the maximum amount of storage the replication system will consume, in this case 4TB. |
| Replication-Secret-Override | Replication-Secret-Override=replicationsecret | Overrides the authentication token used when establishing connections to replication peers.  By default the "Control-Auth" token from the Global configuration group is used. |
| Insecure-Skip-TLS-Verify | Insecure-Skip-TLS-Verify=true | Disables verification and validation of TLS public keys.  TLS is still enabled, but the system will accept any public key presented by a peer. |
| Disable-TLS | Disable-TLS=true | Disables TLS communication between replication peers. Defaults to false (TLS enabled) |
| Key-File | Key-File=/opt/gravwell/etc/replicationkey.pem | Overrides the X509 private key used for negotiating a replication connection.  By default TLS connections use the Global key file. |
| Certificate-File | Certificate-File=/opt/gravwell/etc/replicationcert.pem | Overrides the X509 public key certificate used for negotiating a replication connection.  By default TLS connections use the Global certificate file. |

## レプリケーションエンジンの動作

レプリケーションエンジンは、取り込みおよび検索への影響を最小限に抑えるように設計された、ベストエフォート型の非同期レプリケーションおよび復元システムです。レプリケーションシステムはベストエフォート型のデータ配布を試みますが、タイムリーな割り当てと配布に焦点を当てています。つまり、分割は、先の配布に基づいたガイダンスとともに、配布された先着順で割り当てられます。システムは完全に均一なデータ分散を試みることはなく、より高いスループット（帯域幅、ストレージ、またはCPU）を持つレプリケーションピアは、より少ないピアよりも大きなレプリケーション負荷を受ける可能性があります。データ複製をサポートすることを目的としたGravwellクラスタートポロジーを設計するときは、予期しないバーストや完全に均一ではないデータ分散を可能にするために、複製ストレージを10〜15％オーバープロビジョニングすることをお勧めします。

レプリケーションエンジンは、タグと実際のエントリという2つのコアデータのバックアップを保証します。印刷可能なタグ名のストレージIDへのマッピングは各インデクサーによって個別に維持され、効果的な検索には不可欠です。マップに名前を付けるタグは比較的小さいので、すべてのインデクサーはそのマップ全体を他のすべての複製ピアに複製します。一方、データは一度だけ複製されます。

レプリケーションは、データのエージアウト、移行、およびウェルの分離と連携するように設計されています。インデクサーがデータをコールド・ストレージ・プールに期限切れにするか、または完全に削除すると、データ領域はコールドまたはリモートのストレージ・ピア上の削除済みとしてマークされます。リモートストレージピアは、ノード障害時にどのデータを保持または復元するかを決定するときに、削除、コールドストレージ、および分割期間を使用します。データがインデクサーによって削除済みとしてマークされている場合、インデクサーが失敗してレプリケーションを介して回復したとしても、データは復元されません。以前にコールドとしてマークされていたデータは、復元中にコールドストレージプールに直接戻されます。インデクサーは、レプリケーションを使用して回復するときに、障害発生前とまったく同じ状態に復元する必要があります。

### ベストプラクティス

高可用性Gravwellクラスターの設計とデプロイは、いくつかの基本的なベストプラクティスに従っている限り、非常に簡単です。以下のリストでは、Gravwell管理者がGravwellクラスター・インスタンスをデプロイおよびリカバリーするときに従うべきいくつかのガイドラインを示しています。

1. `Indexer-UUID`インデクサーのグローバルなアイデンティティを表します。IDはノードの存続期間中維持され、障害後に適切に復元されなければなりません。失敗したインデクサーが以前に使用したものとは異なるUUIDを使用した場合、それはレプリケーションクラスター内のまったく新しいメンバーとして解釈され、以前のデータは復元されません。インデクサーに致命的な障害が発生した場合に備えて、インデクサーUUIDをどこか安全な場所に書き留めておくことをお勧めします。
2. 適切な構成を変更すると、レプリケーションの状態に影響する可能性があります。追加のウェルを追加したり、ウェルを削除したりすることは完全に許容できますが、障害の後で復元前にウェル構成を変更すると、レプリケーションエンジンがデータを適切に復元できなくなります。
3. インデクサーが失敗した場合、新しいデータを取り込む前にレプリケーションピアとの接続を確立し、第1レベルのタグ同期を実行できることが非常に重要です。`Connect-Wait-Timeout`パラメーターをゼロに設定して、失敗したインデクサーがレプリケーション接続を確立してタグの復元を実行するまで起動しないようにすることをお勧めします。
4. レプリケーション格納場所は、単一のレプリケーションシステム専用に予約する必要があります。たとえば、複数のインデクサーに同じネットワーク接続ストレージの場所を使用すると、`Storage-Location`レプリケーションが失敗したりデータが破損したりします。

## トラブルシューティング

レプリケーションの問題をデバッグする際の潜在的な問題と解決策

#### 失敗の後、インデクサーはそのデータを復元しませんでした

`Indexer-UUID`オンラインに戻ったときにインデクサーが元の値を維持していることを確認してください。UUIDが変更された場合は、元の値に戻して、インデクサーにすべてのデータを復元するための十分な時間があることを確認してください。変更後の復元に`Indexer-UUID`は、レプリケーションシステムが2つの異なるデータストアをマージするため、かなり時間がかかります。

#### データがレプリケーション保管場所に表示されない
すべての複製ピアに共通の`Control-Auth`（または`Replication-Secret-Override`）値があることを確認してください。ピアが互いに認証できない場合、データを交換しません。

X509証明書が、ホストシステム上のキーストアによって尊重されている有効な認証局（CA）によって署名されていることを確認してください。証明書ストアが有効でない場合は、公開鍵をホストマシンの証明書ストアにインストールするか、または`Insecure-Skip-TLS-Verify`オプションでTLS検証を無効にします。

重要：を介したTLS検証を無効に`Insecure-Skip-TLS-Verify`すると、中間者攻撃への複製が可能になります。攻撃者は飛行中のデータを変更し、ログを破損したり、複製されたデータの活動を隠したりする可能性があります。

指定されたポートでインデクサーが互いに通信することを許可されていることを確認するためにファイアウォールのルールやルーティングACLをチェックしてください。

#### 失敗後、タグマージが失敗したためインデクサーが起動を拒否しました

タグマッピングを復元する前に、障害発生後にインデクサーが取り込みを開始すると、レプリケーションノード上のタグマップをマージできない状態になる可能性があります。タグ付けできないタグエラーが発生した場合は、失敗したノードを手動で復元する方法についてsupport@gravwell.ioにお問い合わせください。

#### 失敗した後、インデクサーがすべてのデータを復元しませんでした
レプリケーションパフォーマンスの低下、ネットワークパフォーマンスの低下、またはレプリケーションノードでのストレージ障害のため、レプリケーションピアがインデクサーに追いつくことができなかった可能性があります。レプリケーションピアが取り込みに追いつくために十分な帯域幅とストレージ容量を持っていることを確認します。ストレージノードが毎秒数百メガバイトのデータを取り込んでいる場合、レプリケーションピアは同じ速度でデータを計算、転送、および保存できる必要があります。

レプリケーションピアに十分なストレージがあることも確認してください。ストレージノードが10TBのコールドデータと1TBのホットデータを保持するように構成されている場合、レプリケーションピアは少なくとも11TBのデータを格納できる必要があります。レプリケーションノードが過負荷になっているか設定が誤っている場合は、古いデータが削除されている可能性があります。

レプリケーションノードとインデクサーのシステム時間が一致していることを確認してください。どちらのシステムも、削除するデータの適格性を判断するために実時間を使用します。インデクサーのシステム時間が正しくない場合は、レプリケーションピアのストレージが不足した場合に備えて、データの削除が優先されます。